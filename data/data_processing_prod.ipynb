{"cells":[{"cell_type":"markdown","metadata":{"id":"8me5UeT-PMiV"},"source":["# **Importamos librerias de interes**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TU-b7dOuPAzF"},"outputs":[],"source":["import random\n","\n","from joblib import dump, load\n","\n","import numpy as np\n","\n","import pandas as pd\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","from sklearn import svm\n","\n","from sklearn.preprocessing import StandardScaler\n","\n","from sklearn.pipeline import make_pipeline\n","\n","import keras\n","\n","import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","\n","from scipy import interpolate\n","\n","from google.colab import drive"]},{"cell_type":"markdown","metadata":{"id":"CFyaHyk9PYOO"},"source":["# **Funciones de trabajo**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gijIfno7PYw8"},"outputs":[],"source":["# Definimos las funciones de transformacion\n","\n","#_______________________________________________________________________________\n","def fun_action_time(df_data):\n","\n","  action_time=df_data['action_time']\n","\n","  action_time=df_data['action_time'].values.reshape(-1,1)\n","\n","  return action_time\n","\n","#_______________________________________________________________________________\n","def fun_delay_time(df_data):\n","\n","  up_time_displaced=pd.concat(\n","                              objs=[  pd.Series([0]) ,   df_data['up_time']  ],\n","                              ignore_index=True\n","                                                            )\n","\n","  delay_time=(df_data['down_time'].values -\n","                              up_time_displaced.iloc[0:-1].values\n","                                                                ).reshape(-1,1)\n","\n","  return delay_time\n","\n","#_______________________________________________________________________________\n","def fun_activity(df_data, aux_dict):\n","\n","  map_dict_activity=aux_dict['map_dict_activity']['map_dict']\n","\n","  vocab_activity=aux_dict['map_dict_activity']['vocab']\n","\n","  layer_activity= tf.keras.layers.StringLookup(\n","                                      vocabulary=vocab_activity,\n","                                      num_oov_indices=1,\n","                                      oov_token='[UNK]', # mapea a 0\n","                                      output_mode='int')\n","\n","  activity_cut=df_data['activity'].apply(lambda x: x[:3])\n","\n","  cat_activity=pd.Series(layer_activity(activity_cut)).apply(lambda x:\n","                                    map_dict_activity[x]).values.reshape(-1,1)\n","\n","  return cat_activity\n","\n","#_______________________________________________________________________________\n","\n","# funcion auxiliar 1\n","def mod_act(str_arr):\n","  if str_arr=='NoChange':\n","    return 0\n","  else:\n","    if ' => ' in str_arr:\n","      str_split=str_arr.split(' => ', maxsplit=1)\n","      return len(str_split[0])+len(str_split[1])\n","    else:\n","      return len(str_arr)\n","\n","# funcion auxiliar 2\n","def cut_act(len_act):\n","  if len_act>=2:\n","    return 2\n","  else:\n","    return len_act\n","\n","def fun_mod_activity(df_data, aux_dict):\n","\n","  map_dict_mod_activity=aux_dict['map_dict_mod_activity']['map_dict']\n","\n","  vocab_mod_activity=aux_dict['map_dict_mod_activity']['vocab']\n","\n","  layer_mod_activity=tf.keras.layers.IntegerLookup(\n","                                  vocabulary=vocab_mod_activity,\n","                                  num_oov_indices=1, # lo mapea a 0\n","                                  output_mode='int',\n","                                  vocabulary_dtype='int64')\n","\n","\n","  activity_mod=df_data['text_change'].apply(mod_act).apply(cut_act)\n","\n","  cat_mod_activity=pd.Series(layer_mod_activity(activity_mod)).apply(\n","                        lambda x: map_dict_mod_activity[x]).values.reshape(-1,1)\n","\n","  return cat_mod_activity\n","\n","#_______________________________________________________________________________\n","\n","def fun_event(df_data, aux_dict):\n","\n","\n","  map_dict_event=aux_dict['map_dict_event']['map_dict']\n","\n","  vocab_event=aux_dict['map_dict_event']['vocab']\n","\n","  layer_event= tf.keras.layers.StringLookup(vocabulary=vocab_event,\n","                                      num_oov_indices=1,\n","                                     output_mode='int')\n","\n","  event=pd.Series(layer_event(df_data['up_event'])).apply(\n","                              lambda x: map_dict_event[x]).values.reshape(-1,1)\n","\n","  return event\n","\n","#_______________________________________________________________________________\n","\n","def fun_cursor_position(df_data):\n","\n","  cursor_position=df_data['cursor_position'].values\n","\n","  x_cursor_position=np.arange(len(cursor_position)).reshape(-1,1)\n","\n","  # definimos el svr\n","\n","  regr_cp = make_pipeline(StandardScaler(), svm.SVR(kernel='linear',C=100\n","                                                            , epsilon=10e-21))\n","\n","  regr_cp.fit(x_cursor_position,\n","              cursor_position)\n","\n","  cursor_position_lin=regr_cp.predict(x_cursor_position)\n","\n","  cursor_position_flat=(cursor_position_lin-cursor_position).reshape(-1,1)\n","\n","  return cursor_position_flat\n","\n","#_______________________________________________________________________________\n","\n","def fun_word_count(df_data):\n","\n","  std_scaler=StandardScaler()\n","\n","  word_count=df_data['word_count'].values\n","\n","  x_word_count=np.arange(len(word_count)).reshape(-1,1)\n","\n","  regr_wc = make_pipeline(StandardScaler(), svm.SVR(kernel='linear',C=100,\n","                                                    epsilon=10e-21))\n","\n","  regr_wc.fit(\n","    x_word_count,\n","    word_count)\n","\n","  word_count_lin=regr_wc.predict(x_word_count)\n","\n","  word_count_flat=(word_count_lin-word_count).reshape(-1,1)\n","\n","  # hacemos la interpolacion\n","\n","  index_interp=np.floor(np.linspace(start=x_word_count[0],\n","                                      stop=x_word_count[-1] ,\n","                                          num=20)).astype(int)\n","\n","  x_intp=np.squeeze(x_word_count[index_interp])\n","\n","  y=np.squeeze(word_count_flat[index_interp])\n","\n","  f = interpolate.interp1d(x_intp, y,'nearest-up')\n","\n","  word_count_flat_intp=f(x_word_count).reshape(-1,1)\n","\n","  return word_count_flat_intp\n","\n","\n","#_______________________________________________________________________________\n","\n","\n","# funcion auxiliar\n","\n","def text_change_format(row):\n","\n","  if ' => ' in row:\n","    return '=>'\n","  else:\n","    return row\n","\n","def fun_text_change(df_data, aux_dict):\n","\n","\n","  map_dict_tc=aux_dict['map_dict_tc']['map_dict']\n","\n","\n","\n","  vocab_tc=aux_dict['map_dict_tc']['vocab']\n","\n","\n","  layer_tc= tf.keras.layers.StringLookup(vocabulary=vocab_tc,\n","                                            num_oov_indices=1,\n","                                                  output_mode='int')\n","\n","\n","\n","  tc_data=pd.Series(\n","          layer_tc(\n","              df_data['text_change'].apply(text_change_format)\n","                                        )\n","                                            ).apply(lambda x:\n","                                            map_dict_tc[x]).values.reshape(-1,1)\n","\n","\n","  return tc_data\n","\n","#_______________________________________________________________________________\n","\n","def fun_len_time(df_data):\n","\n","  max_len_time=8313000\n","\n","  max_ts_len_time=np.maximum(df_data['up_time'].max(),\n","                             df_data['down_time'].max())\n","\n","  return max_ts_len_time/max_len_time\n","\n","\n","\n","#_______________________________________________________________________________\n","\n","def fun_len_cursor_position(df_data):\n","\n","  max_cursor_position=7800\n","\n","  max_ts_cursor_position=df_data['cursor_position'].max()\n","\n","  return max_ts_cursor_position/max_cursor_position\n","\n","\n","\n","#_______________________________________________________________________________\n","\n","def fun_len_word(df_data):\n","\n","  max_len_word=1330\n","\n","  max_ts_len_word=df_data['word_count'].max()\n","\n","  return max_ts_len_word/max_len_word\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"py1cKA4NxYPK"},"outputs":[],"source":["\n","#-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n","\n","def train_data_processing( df_logs, df_scores, id_list):\n","\n","  # definimos algunas cosas\n","\n","  df_train=df_logs[df_logs['id'].isin(id_list)]\n","\n","  df_target=df_scores[df_scores['id'].isin(id_list)]\n","\n","  aux_dict={}\n","\n","  # vamos generando los diccionarios necesarios\n","\n","#-------------------------------------------------------------------------------\n","\n","  activity_cut=df_train['activity'].apply(lambda x: x[:3])\n","\n","  vocab_activity=list(activity_cut.value_counts(normalize=True).index.values)\n","\n","  # definimos la capa\n","\n","  layer_activity= tf.keras.layers.StringLookup(\n","                                      vocabulary=vocab_activity,\n","                                      num_oov_indices=1,\n","                                      oov_token='[UNK]', # mapea a 0\n","                                      output_mode='int')\n","\n","\n","\n","  # extraemos los indices\n","\n","  index=pd.Series(layer_activity(activity_cut)).value_counts(\n","                                                normalize=True).index.values\n","\n","  # la aplicacion de la funcion\n","\n","  fun_map=pd.Series(layer_activity(activity_cut)).value_counts(\n","                                                normalize=True).apply(lambda x:\n","                                                                     np.log(1/x)\n","                                                                      ).cumsum()\n","  # creamos el diccionario\n","\n","  map_dict_activity={}\n","\n","  for element in zip(index,fun_map):\n","    map_dict_activity[element[0]]=element[1]\n","\n","  # agregamos la llave de OOV\n","\n","  if 0 not in map_dict_activity:\n","    max_value= max(map_dict_activity.values())\n","    map_dict_activity[0]=2*max_value\n","\n","\n","  # guardamos\n","\n","  aux_dict['map_dict_activity']={'map_dict':map_dict_activity,\n","                                 'vocab':vocab_activity}\n","\n","#-------------------------------------------------------------------------------\n","\n","  vocab_mod_activity=list(df_train['text_change'].apply(mod_act).apply(cut_act)\n","                                     .value_counts(normalize=True).index.values)\n","\n","  layer_mod_activity=tf.keras.layers.IntegerLookup(\n","                                  vocabulary=vocab_mod_activity,\n","                                  num_oov_indices=1, # lo mapea a 0\n","                                  output_mode='int',\n","                                  vocabulary_dtype='int64')\n","\n","  index=pd.Series(layer_mod_activity(df_train['text_change']\n","                                     .apply(mod_act)\n","                                     .apply(cut_act)\n","                                                              )\n","                                    ).value_counts(normalize=True).index.values\n","\n","  fun_map=pd.Series(layer_mod_activity(df_train['text_change']\n","                                     .apply(mod_act)\n","                                     .apply(cut_act)\n","                                                              )\n","                                    ).value_counts(\n","                           normalize=True).apply(lambda x: np.log(1/x)).cumsum()\n","\n","  map_dict_mod_activity={}\n","\n","  for element in zip(index,fun_map):\n","    map_dict_mod_activity[element[0]]=element[1]\n","\n","\n","  # agregamos la llave de OOV\n","\n","  if 0 not in map_dict_mod_activity:\n","    max_value= max(map_dict_mod_activity.values())\n","    map_dict_activity[0]=2*max_value\n","\n","\n","  aux_dict['map_dict_mod_activity']={'map_dict':map_dict_mod_activity,\n","                                      'vocab':vocab_mod_activity}\n","\n","#-------------------------------------------------------------------------------\n","\n","  df_event=pd.concat(objs=[df_train['down_event'],\n","                           df_train['up_event']\n","                                                    ],axis=0,ignore_index=True)\n","\n","  vocab_event=list(df_event.value_counts(normalize=True).index.values)\n","\n","\n","  if len(vocab_event)>15:\n","    vocab_event=vocab_event[:15]\n","\n","\n","  layer_event= tf.keras.layers.StringLookup(vocabulary=vocab_event,\n","                                      num_oov_indices=1,\n","                                      output_mode='int')\n","\n","  # creamos el diccionario\n","\n","  index=pd.Series(layer_event(df_train['up_event'])).value_counts(\n","                                                    normalize=True).index.values\n","\n","  fun_map=pd.Series(layer_event(df_train['up_event'])).value_counts(\n","                                                    normalize=True).apply(\n","                                                 lambda x: np.log(1/x)).cumsum()\n","\n","\n","  map_dict_event={}\n","\n","  for element in zip(index,fun_map):\n","      map_dict_event[element[0]]=element[1]\n","\n","\n","\n","  if 0 not in map_dict_event:\n","    max_value= max(map_dict_event.values())\n","    map_dict_event[0]=2*max_value\n","\n","\n","\n","\n","  aux_dict['map_dict_event']={'map_dict':map_dict_event,\n","                              'vocab':vocab_event}\n","\n","\n","#-------------------------------------------------------------------------------\n","\n","  tc_transf=df_train['text_change'].apply(text_change_format)\n","\n","  vocab_tc=list(tc_transf.value_counts(normalize=True).index.values)\n","\n","  if len(vocab_tc)>12:\n","    vocab_tc=vocab_tc[:12]\n","\n","  layer_tc= tf.keras.layers.StringLookup(vocabulary=vocab_tc,\n","                                            num_oov_indices=1,\n","                                                  output_mode='int')\n","\n","\n","  index=pd.Series(layer_tc(df_train['text_change'].apply(\n","                text_change_format))).value_counts(normalize=True).index.values\n","\n","  fun_map=pd.Series(layer_tc(df_train['text_change'].apply(\n","        text_change_format))).value_counts(normalize=True).apply(\n","                                                 lambda x: np.log(1/x)).cumsum()\n","\n","  # creamos el dic\n","\n","  map_dict_tc={}\n","\n","  for element in zip(index,fun_map):\n","    map_dict_tc[element[0]]=element[1]\n","\n","  if 0 not in map_dict_tc:\n","    max_value= max(map_dict_tc.values())\n","    map_dict_tc[0]=2*max_value\n","\n","\n","\n","  aux_dict['map_dict_tc']={'map_dict':map_dict_tc,\n","                           'vocab':vocab_tc }\n","\n","#-------------------------------------------------------------------------------\n","  # construimos el diccionario de data\n","\n","  dict_train={}\n","\n","  for id in id_list:\n","    df_instance=df_train[df_train['id']==id]\n","\n","  # concatenamos la data\n","\n","    dyn_data=np.hstack(\n","                      tup=(\n","\n","    fun_action_time(df_instance),\n","    fun_delay_time(df_instance),\n","    fun_activity(df_instance, aux_dict),\n","    fun_mod_activity(df_instance, aux_dict),\n","    fun_event(df_instance, aux_dict),\n","    fun_cursor_position(df_instance),\n","    fun_word_count(df_instance),\n","    fun_text_change(df_instance, aux_dict)\n","                                    )\n","                                                  )\n","\n","    stat_data=np.hstack(\n","\n","                          tup=(\n","    fun_len_time(df_instance),\n","    fun_len_cursor_position(df_instance),\n","    fun_len_word(df_instance)\n","\n","                                        )\n","\n","                              )\n","\n","    target=df_target[df_target['id']==id]['score'].values\n","\n","    dict_train[id]={\n","\n","        'dyn_data':dyn_data,\n","        'stat_data':stat_data,\n","        'target':target\n","                                }\n","\n","\n","\n","#-------------------------------------------------------------------------------\n","  # hacemos el scaling y pading\n","\n","  list_scaler=[]\n","\n","  for k in range(8):\n","\n","    list_seq=[]\n","    for id in dict_train:\n","      seq=list(dict_train[id]['dyn_data'][:,k])\n","      list_seq=list_seq+seq\n","\n","    list_scaler.append(\n","                      MinMaxScaler(feature_range=(0, 5))\n","                      .fit(np.array(list_seq).reshape(-1,1))\n","                                                              )\n","\n","  # aplicamos el scaler para cada secuencia en cada columna de dyn data\n","\n","  for id in dict_train:\n","\n","    dyn_data=dict_train[id]['dyn_data']\n","\n","    dict_train[id]['dyn_data']=np.hstack(tup=tuple(\n","\n","                  [ list_scaler[k].transform(dyn_data[:,k].reshape(-1,1))\n","                                                            for k in range(8) ]\n","\n","                                              )\n","                                                  )\n","\n","  # aplicamos pading sobre la data\n","  # determianmos la longitud maxima de las secuencias\n","\n","  max_len=int(df_train['event_id'].max()*1.05)\n","\n","\n","  aux_dict['max_len']=max_len\n","\n","  list_data=[]\n","\n","  for column in range(8):\n","\n","    list_seq=[]\n","\n","    for id in dict_train:\n","      list_seq.append(dict_train[id]['dyn_data'][:,column])\n","\n","    list_data.append(list_seq)\n","\n","\n","  # aplicamos padding a cada columna y juntamos las columns\n","  pad_list_data=[tf.keras.utils.pad_sequences(\n","                                            sequences=column,\n","                                              maxlen=max_len,\n","                                              dtype='float64',\n","                                              padding='post',\n","                                              truncating='pre',\n","                                              value=-1\n","\n","                                                ).reshape(len(dict_train),-1,1)\n","                                                    for column in list_data\n","                                                              ]\n","\n","\n","\n","  dyn_train_data=np.concatenate(tuple(pad_list_data),axis=2)\n","  # preparamos la data estatica\n","\n","  stat_train_data=np.vstack(\n","\n","  tup=tuple([dict_train[id]['stat_data'] for id in dict_train])\n","                                                                )\n","\n","  # preparamos los targets\n","\n","  target_train_data=df_target['score'].values.reshape(-1,1)\n","\n","  return  dyn_train_data, stat_train_data, target_train_data, aux_dict\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gff3ZJeOarhg"},"outputs":[],"source":["#-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n","\n","def test_data_processing( df_logs, df_scores, id_list,aux_dict):\n","\n","  # definimos algunas cosas\n","\n","  df_test=df_logs[df_logs['id'].isin(id_list)]\n","\n","  df_target=df_scores[df_scores['id'].isin(id_list)]\n","\n","#-------------------------------------------------------------------------------\n","  # construimos el diccionario de data\n","\n","  dict_test={}\n","\n","  for id in id_list:\n","    df_instance=df_test[df_test['id']==id]\n","\n","  # concatenamos la data\n","\n","    dyn_data=np.hstack(\n","                      tup=(\n","\n","    fun_action_time(df_instance),\n","    fun_delay_time(df_instance),\n","    fun_activity(df_instance, aux_dict),\n","    fun_mod_activity(df_instance, aux_dict),\n","    fun_event(df_instance, aux_dict),\n","    fun_cursor_position(df_instance),\n","    fun_word_count(df_instance),\n","    fun_text_change(df_instance, aux_dict)\n","                                    )\n","                                                  )\n","\n","    stat_data=np.hstack(\n","\n","                          tup=(\n","    fun_len_time(df_instance),\n","    fun_len_cursor_position(df_instance),\n","    fun_len_word(df_instance)\n","\n","                                        )\n","\n","                              )\n","\n","    target=df_target[df_target['id']==id]['score'].values\n","\n","    dict_test[id]={\n","\n","        'dyn_data':dyn_data,\n","        'stat_data':stat_data,\n","        'target':target\n","                                }\n","\n","\n","\n","#-------------------------------------------------------------------------------\n","  # hacemos el scaling y pading\n","\n","  list_scaler=[]\n","\n","  for k in range(8):\n","\n","    list_seq=[]\n","    for id in dict_test:\n","      seq=list(dict_test[id]['dyn_data'][:,k])\n","      list_seq=list_seq+seq\n","\n","    list_scaler.append(\n","                      MinMaxScaler(feature_range=(0, 5))\n","                      .fit(np.array(list_seq).reshape(-1,1))\n","                                                              )\n","\n","  # aplicamos el scaler para cada secuencia en cada columna de dyn data\n","\n","  for id in dict_test:\n","\n","    dyn_data=dict_test[id]['dyn_data']\n","\n","    dict_test[id]['dyn_data']=np.hstack(tup=tuple(\n","\n","                  [ list_scaler[k].transform(dyn_data[:,k].reshape(-1,1))\n","                                                            for k in range(8) ]\n","\n","                                              )\n","                                                  )\n","\n","  # aplicamos pading sobre la data\n","  # determianmos la longitud maxima de las secuencias\n","\n","  max_len=aux_dict['max_len']\n","\n","  list_data=[]\n","\n","  for column in range(8):\n","\n","    list_seq=[]\n","\n","    for id in dict_test:\n","      list_seq.append(dict_test[id]['dyn_data'][:,column])\n","\n","    list_data.append(list_seq)\n","\n","\n","  # aplicamos padding a cada columna y juntamos las columns\n","  pad_list_data=[tf.keras.utils.pad_sequences(\n","                                            sequences=column,\n","                                              maxlen=max_len,\n","                                              dtype='float64',\n","                                              padding='post',\n","                                              truncating='pre',\n","                                              value=-1\n","\n","                                                ).reshape(len(dict_test),-1,1)\n","                                                       for column in list_data\n","                                                              ]\n","\n","\n","  dyn_test_data=np.concatenate(tuple(pad_list_data),axis=2)\n","\n","  # preparamos la data estatica\n","\n","  stat_test_data=np.vstack(\n","\n","  tup=tuple([dict_test[id]['stat_data'] for id in dict_test])\n","                                                                )\n","\n","  # preparamos los targets\n","\n","  target_test_data=df_target['score'].values.reshape(-1,1)\n","\n","  return  dyn_test_data, stat_test_data, target_test_data"]},{"cell_type":"markdown","metadata":{"id":"6WdGyB9KPgIg"},"source":["# **importamos la data**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55114,"status":"ok","timestamp":1706733108638,"user":{"displayName":"pacha","userId":"03278805039111223622"},"user_tz":240},"id":"HRAApsuvPi0L","outputId":"0204460e-2ef7-436b-f9ed-225981b5d6c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# conectamos al drive\n","drive.mount('/content/drive')\n","\n","# Definimos el directorio\n","dir_data_kaggle='./drive/MyDrive/lwpwq/data'\n","\n","# extraemos la data\n","\n","df_logs=pd.read_csv(filepath_or_buffer=dir_data_kaggle+'/train_logs.csv')\n","\n","df_scores=pd.read_csv(filepath_or_buffer=dir_data_kaggle+'/train_scores.csv')"]},{"cell_type":"markdown","metadata":{"id":"XqtrFVjSxon9"},"source":["# **Definimos el split**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CYYaW989xs7j"},"outputs":[],"source":["sample_id=list(df_logs['id'].unique())\n","\n","train_size=int(len(df_logs['id'].unique())*0.9)\n","\n","id_list_train= random.sample(sample_id, k=train_size)\n","\n","complement_sample_id=[id for id in sample_id if id not in id_list_train]\n","\n","id_list_test= complement_sample_id"]},{"cell_type":"markdown","metadata":{"id":"9pgA945r2atL"},"source":["# **Pre procesamos la data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ykniwuuu2Z8t"},"outputs":[],"source":["\n","\n","dyn_train_data, stat_train_data, target_train_data, aux_dict= train_data_processing(df_logs,\n","                                                                                    df_scores,\n","                                                                                    id_list_train)\n","\n","\n","dyn_test_data, stat_test_data, target_test_data=test_data_processing( df_logs,\n","                                                                     df_scores,\n","                                                                  id_list_test,\n","                                                                      aux_dict)"]},{"cell_type":"markdown","metadata":{"id":"vPTQIbac4D3v"},"source":["# **Exportamos la data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"il1pGKuL4IiZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706742040917,"user_tz":240,"elapsed":35114,"user":{"displayName":"pacha","userId":"03278805039111223622"}},"outputId":"267f2543-7d02-454a-feb3-7ccae6a31fa3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['./drive/MyDrive/lwpwq/data/prod_dict.joblib']"]},"metadata":{},"execution_count":8}],"source":["np.save(dir_data_kaggle+'/dyn_train_data_prod.npy', dyn_train_data)\n","np.save(dir_data_kaggle+'/dyn_test_data_prod.npy', dyn_test_data)\n","\n","\n","np.save(dir_data_kaggle+'/stat_train_data_prod.npy', stat_train_data)\n","np.save(dir_data_kaggle+'/stat_test_data_prod.npy', stat_test_data)\n","\n","\n","np.save(dir_data_kaggle+'/target_train_data_prod.npy', target_train_data)\n","np.save(dir_data_kaggle+'/target_test_data_prod.npy', target_test_data)\n","\n","dump(aux_dict, dir_data_kaggle+'/prod_dict.joblib')"]}],"metadata":{"colab":{"collapsed_sections":["8me5UeT-PMiV"],"provenance":[],"authorship_tag":"ABX9TyOe7CnUIbw//F/7rP8qDifT"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}